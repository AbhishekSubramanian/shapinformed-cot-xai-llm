{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrxesWU_B3EG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm.auto import tqdm\n",
        "import time\n",
        "import gc\n",
        "import ast # To parse the string representation of the list/tuples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StM1RoG6B34F"
      },
      "outputs": [],
      "source": [
        "# --- Configuration ---\n",
        "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "access_token = \"\"\n",
        "\n",
        "# Max tokens for the generated *SHAP-informed explanation*\n",
        "max_shap_explanation_length = 50\n",
        "# Input file path (output from the previous step)\n",
        "input_file_with_explanations = '/content/Llama_3_8B_Instruct_explanations.csv'\n",
        "\n",
        "top_n_shap_tokens = 10\n",
        "\n",
        "# --- Original Label Mapping (Crucial for interpreting SHAP values) ---\n",
        "label_map = {\n",
        "    0: 'entailment',\n",
        "    1: 'neutral',\n",
        "    2: 'contradiction'\n",
        "}\n",
        "# Create inverse map: label name -> index\n",
        "label_to_index = {v: k for k, v in label_map.items()}\n",
        "\n",
        "# --- Load Model and Tokenizer (if not already loaded) ---\n",
        "# Re-use the loading code from the previous script if needed,\n",
        "# ensuring the same configuration (dtype, device_map, pad_token).\n",
        "# Example minimal reload:\n",
        "print(\"Checking/Loading model and tokenizer...\")\n",
        "try:\n",
        "    model\n",
        "    tokenizer\n",
        "    print(\"Model and tokenizer already loaded.\")\n",
        "    model.eval() # Ensure it's in eval mode\n",
        "except NameError:\n",
        "    print(\"Loading model and tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=access_token)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"left\"\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        token=access_token,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    model.eval()\n",
        "    print(\"Model and tokenizer loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-KRvtI1B3z8"
      },
      "outputs": [],
      "source": [
        "# --- Helper Function for Generation (Use the improved one) ---\n",
        "def generate_model_explanation(prompt, max_new_tokens):\n",
        "    \"\"\"Generates text using the loaded model (incorporate sampling/penalty).\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=tokenizer.model_max_length - max_new_tokens).to(model.device)\n",
        "    attention_mask = inputs.attention_mask\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs.input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id, # Important for stopping\n",
        "            # --- Use Improved Generation Params ---\n",
        "            do_sample=True,\n",
        "            temperature=0.1, # Or your preferred value\n",
        "            top_k=50,        # Or your preferred value\n",
        "            repetition_penalty=1.15 # Or your preferred value\n",
        "            # --- End Improved Params ---\n",
        "        )\n",
        "\n",
        "    input_length = inputs.input_ids.shape[1]\n",
        "    generated_ids = outputs[0, input_length:]\n",
        "    explanation = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "    explanation = explanation.strip()\n",
        "\n",
        "    del inputs, outputs\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return explanation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSYYJMQtB3wB"
      },
      "outputs": [],
      "source": [
        "# --- Helper Function to Process SHAP String ---\n",
        "def get_top_shap_tokens(shap_str, pred_label, label_to_index_map, top_n=10):\n",
        "    \"\"\"Parses SHAP string and returns top N tokens for the predicted label.\"\"\"\n",
        "    try:\n",
        "        # Safely parse the string representation\n",
        "        shap_data = ast.literal_eval(shap_str)\n",
        "    except (ValueError, SyntaxError, TypeError) as e:\n",
        "        print(f\"Warning: Could not parse SHAP string: {e}\\nString: {shap_str[:100]}...\")\n",
        "        return \"Error parsing SHAP\" # Return placeholder on error\n",
        "\n",
        "    if not isinstance(shap_data, list) or not shap_data:\n",
        "        return \"No SHAP data\"\n",
        "\n",
        "    try:\n",
        "        # Get the index corresponding to the predicted label\n",
        "        pred_index = label_to_index_map.get(pred_label)\n",
        "        if pred_index is None:\n",
        "            print(f\"Warning: Predicted label '{pred_label}' not found in label_to_index map.\")\n",
        "            return \"Unknown label index\"\n",
        "\n",
        "        # Calculate importance score (absolute SHAP value for the predicted class)\n",
        "        token_importance = []\n",
        "        for item in shap_data:\n",
        "             # Ensure item is a tuple/list with at least two elements (token, values_list)\n",
        "            if isinstance(item, (list, tuple)) and len(item) >= 2:\n",
        "                token = item[0]\n",
        "                values = item[1]\n",
        "                 # Ensure values is a list/tuple and index is valid\n",
        "                if isinstance(values, (list, tuple)) and len(values) > pred_index:\n",
        "                    # Use abs value of SHAP for the predicted class as importance\n",
        "                    importance = abs(values[pred_index])\n",
        "                    token_importance.append((token, importance))\n",
        "                else:\n",
        "                    # Handle cases where values format is unexpected\n",
        "                    # print(f\"Warning: Unexpected SHAP values format for token '{token}': {values}\")\n",
        "                    pass # Optionally skip or assign default importance\n",
        "            else:\n",
        "                # Handle cases where item format is unexpected\n",
        "                # print(f\"Warning: Unexpected item format in SHAP data: {item}\")\n",
        "                pass # Optionally skip\n",
        "\n",
        "\n",
        "        # Sort by importance (descending)\n",
        "        token_importance.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Get top N tokens (the actual token strings)\n",
        "        top_tokens = [item[0] for item in token_importance[:top_n]]\n",
        "\n",
        "        # Return as a comma-separated string for the prompt\n",
        "        return \", \".join(top_tokens)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing SHAP data for label '{pred_label}': {e}\")\n",
        "        return \"Error processing SHAP\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPwDolnWB3sv"
      },
      "outputs": [],
      "source": [
        "# --- Load Data ---\n",
        "print(f\"Loading results from {input_file_with_explanations}...\")\n",
        "try:\n",
        "    df_correct = pd.read_csv(input_file_with_explanations)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {input_file_with_explanations}\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"Error loading CSV: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Ensure 'shap_value' column exists\n",
        "if 'shap_value' not in df_correct.columns:\n",
        "    print(f\"Error: 'shap_value' column not found in {input_file_with_explanations}.\")\n",
        "    exit()\n",
        "# Ensure 'pred_label' column exists\n",
        "if 'pred_label' not in df_correct.columns:\n",
        "     print(f\"Error: 'pred_label' column not found in {input_file_with_explanations}.\")\n",
        "     exit()\n",
        "\n",
        "\n",
        "print(f\"Loaded {len(df_correct)} samples.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4ZdBn5pB3pt",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# --- Generate SHAP-informed Explanations ---\n",
        "model_explanations_shap = []\n",
        "print(f\"Generating SHAP-informed explanations (top {top_n_shap_tokens} tokens, max_new_tokens={max_shap_explanation_length})...\")\n",
        "start_time = time.time()\n",
        "\n",
        "for index, row in tqdm(df_correct.iterrows(), total=df_correct.shape[0], desc=\"Generating SHAP Explanations\"):\n",
        "    premise = row['premise']\n",
        "    hypothesis = row['hypothesis']\n",
        "    predicted_label = row['pred_label']\n",
        "    shap_str = row['shap_value']\n",
        "\n",
        "    # Get the most influential tokens based on SHAP for the predicted class\n",
        "    important_tokens_str = get_top_shap_tokens(shap_str, predicted_label, label_to_index, top_n=top_n_shap_tokens)\n",
        "\n",
        "    # Handle cases where SHAP processing failed\n",
        "    if \"Error\" in important_tokens_str or \"No SHAP data\" in important_tokens_str or \"Unknown label\" in important_tokens_str:\n",
        "        shap_explanation = f\"Could not generate SHAP explanation ({important_tokens_str}).\"\n",
        "        print(f\"Skipping SHAP explanation for index {index} due to SHAP processing issue: {important_tokens_str}\")\n",
        "    elif not important_tokens_str:\n",
        "         shap_explanation = \"Could not generate SHAP explanation (No important tokens found).\"\n",
        "         print(f\"Skipping SHAP explanation for index {index} due to no important tokens found.\")\n",
        "    else:\n",
        "        shap_prompt = f\"\"\"Given [Premise: {premise}\n",
        "                  Hypothesis: {hypothesis}\n",
        "                  Classification: {predicted_label}]\n",
        "                  These tokens were important: {important_tokens_str}.\n",
        "                  In exactly one sentence, why is this classification correct?\"\"\"\n",
        "\n",
        "        # Generate the SHAP-informed explanation\n",
        "        shap_explanation = generate_model_explanation(shap_prompt, max_new_tokens=max_shap_explanation_length)\n",
        "\n",
        "    model_explanations_shap.append(shap_explanation)\n",
        "    print(shap_explanation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEPn3qAoCwgr"
      },
      "outputs": [],
      "source": [
        "# --- Add Explanations to DataFrame ---\n",
        "df_correct['shap_model_explanation'] = model_explanations_shap\n",
        "print(\"\\nFinished generating SHAP-informed explanations.\")\n",
        "end_time = time.time()\n",
        "print(f\"Total time taken: {end_time - start_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFebKMXmB3m_"
      },
      "outputs": [],
      "source": [
        "# --- Save Updated DataFrame ---\n",
        "output_file = 'Llama_3_8B_Instruct_explanations_final.csv'\n",
        "print(f\"\\nSaving updated DataFrame to {output_file}...\")\n",
        "try:\n",
        "    df_correct.to_csv(output_file, index=False)\n",
        "    print(\"DataFrame saved successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving CSV: {e}\")\n",
        "\n",
        "print(\"\\nScript finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlbCA58OFoeb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}