{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paWU5Q2D9O80"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm.auto import tqdm\n",
        "import time\n",
        "import gc  # Garbage collector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WklxOczS9P2U"
      },
      "outputs": [],
      "source": [
        "# --- Configuration ---\n",
        "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "access_token = \"\"  # Replace with your token\n",
        "\n",
        "# Max tokens for the generated *explanation* (adjust as needed)\n",
        "max_explanation_length = 75\n",
        "# Path to your saved correct results (adjust if needed)\n",
        "correct_results_file = '/content/llama3_nli_analysis_500_shap_correct.csv'\n",
        "# Output file path\n",
        "output_file = 'results_correct_with_explanations.csv'\n",
        "\n",
        "# --- Load Model and Tokenizer ---\n",
        "print(\"Loading model and tokenizer...\")\n",
        "# Ensure tokenizer is loaded correctly\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, token=access_token)\n",
        "if tokenizer.pad_token is None:\n",
        "    print(\"Setting pad_token to eos_token\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\" # Important for Causal LM generation\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    token=access_token,\n",
        "    torch_dtype=torch.float16, # Use float16 for efficiency\n",
        "    device_map=\"auto\" # Automatically distribute across available devices (GPU/CPU)\n",
        ")\n",
        "model.eval() # Set model to evaluation mode\n",
        "print(\"Model and tokenizer loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9x-ZfHv9Px9"
      },
      "outputs": [],
      "source": [
        "# --- Helper Function for Generation ---\n",
        "def generate_model_explanation(prompt, max_new_tokens):\n",
        "    \"\"\"Generates text using the loaded model.\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=tokenizer.model_max_length - max_new_tokens).to(model.device)\n",
        "\n",
        "    # Ensure attention_mask is passed if padding is used\n",
        "    attention_mask = inputs.attention_mask\n",
        "\n",
        "    # Clear GPU cache before generation if using GPU\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Generate output\n",
        "    with torch.no_grad():\n",
        "      outputs = model.generate(\n",
        "        inputs.input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id, # Make sure EOS token is considered\n",
        "\n",
        "        # --- CHANGES START HERE ---\n",
        "        do_sample=True,         # Enable sampling\n",
        "        temperature=0.1,          # Adjust temperature (e.g., 0.-0.9)\n",
        "        top_k=50,               # Consider top K tokens\n",
        "        # top_p=0.9,            # Alternatively, use top_p nucleus sampling\n",
        "        repetition_penalty=1.15 # Penalize repetition (e.g., 1.1-1.2)\n",
        "        # --- CHANGES END HERE ---\n",
        "    )\n",
        "\n",
        "    # Decode the generated tokens, skipping special tokens and the input prompt\n",
        "    # We want only the newly generated part\n",
        "    input_length = inputs.input_ids.shape[1]\n",
        "    generated_ids = outputs[0, input_length:]\n",
        "    explanation = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "    # Basic cleaning\n",
        "    explanation = explanation.strip()\n",
        "\n",
        "    # Clear GPU cache after generation\n",
        "    del inputs, outputs\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect() # Force garbage collection\n",
        "\n",
        "    return explanation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8a0oCbTA9Pt0"
      },
      "outputs": [],
      "source": [
        "# --- Load Data ---\n",
        "print(f\"Loading correct results from {correct_results_file}...\")\n",
        "try:\n",
        "    df_correct = pd.read_csv(correct_results_file)\n",
        "    # Handle potential loading issues if 'shap_value' was complex\n",
        "    if 'shap_value' in df_correct.columns:\n",
        "         # Example: If shap_value needs literal_eval, uncomment below\n",
        "         # import ast\n",
        "         # df_correct['shap_value'] = df_correct['shap_value'].apply(ast.literal_eval)\n",
        "         pass # Add specific handling if needed based on how shap_value was saved\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {correct_results_file}\")\n",
        "    print(\"Please ensure the file exists or modify the 'correct_results_file' variable.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"Error loading CSV: {e}\")\n",
        "    exit()\n",
        "\n",
        "print(f\"Loaded {len(df_correct)} correctly classified samples.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XK30-JD9Pp6"
      },
      "outputs": [],
      "source": [
        "# --- Generate Explanations ---\n",
        "model_explanations = []\n",
        "print(f\"Generating model explanations (max_new_tokens={max_explanation_length})...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Use tqdm for progress bar\n",
        "for index, row in tqdm(df_correct.iterrows(), total=df_correct.shape[0], desc=\"Generating Explanations\"):\n",
        "    premise = row['premise']\n",
        "    hypothesis = row['hypothesis']\n",
        "    # Use 'pred_label' as it's confirmed correct for this dataframe\n",
        "    predicted_label = row['pred_label']\n",
        "\n",
        "    # Construct the prompt for explanation\n",
        "    explanation_prompt = f\"\"\"Premise: {premise}\n",
        "                             Hypothesis: {hypothesis}\n",
        "                             Classification: {predicted_label}\n",
        "                             Brief Reasoning for this classification (Max 25 words):\"\"\"\n",
        "\n",
        "    # Generate the explanation\n",
        "    explanation = generate_model_explanation(explanation_prompt, max_new_tokens=max_explanation_length)\n",
        "    model_explanations.append(explanation)\n",
        "    print(explanation)\n",
        "\n",
        "    # Optional: Print progress periodically\n",
        "    print(f\"Processed {index + 1}/{len(df_correct)} samples...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3qTDEAe9Pmf"
      },
      "outputs": [],
      "source": [
        "# --- Add Explanations to DataFrame ---\n",
        "df_correct['baseline_model_explanation'] = model_explanations\n",
        "print(\"\\nFinished generating explanations.\")\n",
        "end_time = time.time()\n",
        "print(f\"Total time taken: {end_time - start_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuQxO8Wo9Pg4"
      },
      "outputs": [],
      "source": [
        "# --- Save Updated DataFrame ---\n",
        "output_file = 'Llama_3_8B_Instruct_explanations.csv'\n",
        "print(f\"\\nSaving updated DataFrame to {output_file}...\")\n",
        "try:\n",
        "    df_correct.to_csv(output_file, index=False)\n",
        "    print(\"DataFrame saved successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving CSV: {e}\")\n",
        "\n",
        "print(\"\\nScript finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7otcCOSUBDRs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}