{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "64cb653f-fd01-4b25-b7b9-a149f6991851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "import re\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from claude_bedrock import call_claude_on_bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f8abae8f-3460-4272-bee5-5dc03facaf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fname = 'Llama_3_8b_inst_explanations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d3e0ddb8-0d2b-46c1-89bb-6aefa702e786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>gt_label</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>shap_value</th>\n",
       "      <th>human_explanation</th>\n",
       "      <th>baseline_model_explanation</th>\n",
       "      <th>shap_model_explanation</th>\n",
       "      <th>cotfull_response</th>\n",
       "      <th>cot_explanations</th>\n",
       "      <th>cot_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Two women are embracing while holding to go pa...</td>\n",
       "      <td>The men are fighting outside a deli.</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>[('', [0.0, 0.0, 0.0]), ('Prem', [-0.587972005...</td>\n",
       "      <td>In the first sentence there is an action of af...</td>\n",
       "      <td>This is an example of a visual contradiction, ...</td>\n",
       "      <td>**INPUT FORMAT**\\n    &lt;premise&gt;\\n    [premise]...</td>\n",
       "      <td>**SAMPLE INPUT**\\n    &lt;premise&gt;\\n    Two women...</td>\n",
       "      <td>The men are fighting outside a deli.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Two young children in blue jerseys, one with t...</td>\n",
       "      <td>Two kids in numbered jerseys wash their hands.</td>\n",
       "      <td>entailment</td>\n",
       "      <td>entailment</td>\n",
       "      <td>[('', [0.0, 0.0, 0.0]), ('Prem', [-0.7734375, ...</td>\n",
       "      <td>Young children are kids. Jerseys with number 9...</td>\n",
       "      <td>The premise directly states that two kids in n...</td>\n",
       "      <td>**INPUT FORMAT**\\n    &lt;premise&gt;\\n    [premise]...</td>\n",
       "      <td>**SAMPLE INPUT**\\n    &lt;premise&gt;\\n    Two young...</td>\n",
       "      <td>The hypothesis is entailed by the premise beca...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A man selling donuts to a customer during a wo...</td>\n",
       "      <td>A man selling donuts to a customer.</td>\n",
       "      <td>entailment</td>\n",
       "      <td>entailment</td>\n",
       "      <td>[('', [0.0, 0.0, 0.0]), ('Prem', [-0.849609375...</td>\n",
       "      <td>A man selling donuts is selling donuts.</td>\n",
       "      <td>The sentence describes a specific action and s...</td>\n",
       "      <td>**INPUT FORMAT**\\n    &lt;premise&gt;\\n    [premise]...</td>\n",
       "      <td>**SAMPLE INPUT**\\n    &lt;premise&gt;\\n    A man sel...</td>\n",
       "      <td>The premise states that a man is selling donut...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Two young boys of opposing teams play football...</td>\n",
       "      <td>boys play football</td>\n",
       "      <td>entailment</td>\n",
       "      <td>entailment</td>\n",
       "      <td>[('', [0.0, 0.0, 0.0]), ('Prem', [0.505859375,...</td>\n",
       "      <td>If the boys of opposing teams play football th...</td>\n",
       "      <td>The premise states that two young boys are pla...</td>\n",
       "      <td>**INPUT FORMAT**\\n    &lt;premise&gt;\\n    [premise]...</td>\n",
       "      <td>**SAMPLE INPUT**\\n    &lt;premise&gt;\\n    Two young...</td>\n",
       "      <td>Boys playing football is entailed by the premise.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Two young boys of opposing teams play football...</td>\n",
       "      <td>dog eats out of bowl</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>[('', [0.0, 0.0, 0.0]), ('Prem', [-0.196451822...</td>\n",
       "      <td>Boys are not dogs.</td>\n",
       "      <td>The premise is a statement about two people pl...</td>\n",
       "      <td>**INPUT FORMAT**\\n    &lt;premise&gt;\\n    [premise]...</td>\n",
       "      <td>**SAMPLE INPUT**\\n    &lt;premise&gt;\\n    Two young...</td>\n",
       "      <td>The hypothesis is neutral because it does not ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             premise  \\\n",
       "0  Two women are embracing while holding to go pa...   \n",
       "1  Two young children in blue jerseys, one with t...   \n",
       "2  A man selling donuts to a customer during a wo...   \n",
       "3  Two young boys of opposing teams play football...   \n",
       "4  Two young boys of opposing teams play football...   \n",
       "\n",
       "                                       hypothesis       gt_label  \\\n",
       "0            The men are fighting outside a deli.  contradiction   \n",
       "1  Two kids in numbered jerseys wash their hands.     entailment   \n",
       "2             A man selling donuts to a customer.     entailment   \n",
       "3                              boys play football     entailment   \n",
       "4                            dog eats out of bowl  contradiction   \n",
       "\n",
       "      pred_label                                         shap_value  \\\n",
       "0  contradiction  [('', [0.0, 0.0, 0.0]), ('Prem', [-0.587972005...   \n",
       "1     entailment  [('', [0.0, 0.0, 0.0]), ('Prem', [-0.7734375, ...   \n",
       "2     entailment  [('', [0.0, 0.0, 0.0]), ('Prem', [-0.849609375...   \n",
       "3     entailment  [('', [0.0, 0.0, 0.0]), ('Prem', [0.505859375,...   \n",
       "4  contradiction  [('', [0.0, 0.0, 0.0]), ('Prem', [-0.196451822...   \n",
       "\n",
       "                                   human_explanation  \\\n",
       "0  In the first sentence there is an action of af...   \n",
       "1  Young children are kids. Jerseys with number 9...   \n",
       "2            A man selling donuts is selling donuts.   \n",
       "3  If the boys of opposing teams play football th...   \n",
       "4                                 Boys are not dogs.   \n",
       "\n",
       "                          baseline_model_explanation  \\\n",
       "0  This is an example of a visual contradiction, ...   \n",
       "1  The premise directly states that two kids in n...   \n",
       "2  The sentence describes a specific action and s...   \n",
       "3  The premise states that two young boys are pla...   \n",
       "4  The premise is a statement about two people pl...   \n",
       "\n",
       "                              shap_model_explanation  \\\n",
       "0  **INPUT FORMAT**\\n    <premise>\\n    [premise]...   \n",
       "1  **INPUT FORMAT**\\n    <premise>\\n    [premise]...   \n",
       "2  **INPUT FORMAT**\\n    <premise>\\n    [premise]...   \n",
       "3  **INPUT FORMAT**\\n    <premise>\\n    [premise]...   \n",
       "4  **INPUT FORMAT**\\n    <premise>\\n    [premise]...   \n",
       "\n",
       "                                    cotfull_response  \\\n",
       "0  **SAMPLE INPUT**\\n    <premise>\\n    Two women...   \n",
       "1  **SAMPLE INPUT**\\n    <premise>\\n    Two young...   \n",
       "2  **SAMPLE INPUT**\\n    <premise>\\n    A man sel...   \n",
       "3  **SAMPLE INPUT**\\n    <premise>\\n    Two young...   \n",
       "4  **SAMPLE INPUT**\\n    <premise>\\n    Two young...   \n",
       "\n",
       "                                    cot_explanations   cot_label  \n",
       "0               The men are fighting outside a deli.  entailment  \n",
       "1  The hypothesis is entailed by the premise beca...  entailment  \n",
       "2  The premise states that a man is selling donut...  entailment  \n",
       "3  Boys playing football is entailed by the premise.  entailment  \n",
       "4  The hypothesis is neutral because it does not ...     neutral  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(f'{fname}.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c69d24c-383d-4078-b723-9271bb03c253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0f6ee161-a272-484e-a0c3-86961995624a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUDGE_PROMPT =  f\"\"\"\n",
    "# You are given a premise, a hypothesis, few human-written explanations, and a model-generated explanation.\n",
    "\n",
    "# The model has generated \n",
    "# Evaluate the model explanation based on:\n",
    "# - Faithfulness to the premise and hypothesis\n",
    "# - Clarity and ease of understanding\n",
    "# - Completeness of reasoning\n",
    "# - Overall similarity to human explanations\n",
    "\n",
    "# Assign a score from 1 to 5:\n",
    "# - 5: Excellent — Faithful, clear, complete, and closely matches human explanations.\n",
    "# - 4: Good — Mostly correct with minor issues.\n",
    "# - 3: Average — Partially correct but missing important reasoning.\n",
    "# - 2: Poor — Significant mistakes or confusion.\n",
    "# - 1: Very Poor — Wrong, misleading, or irrelevant.\n",
    "\n",
    "# After scoring, briefly explain your reasoning in 2–4 sentences.\n",
    "\n",
    "# ---\n",
    "\n",
    "# Premise:\n",
    "# {{premise}}\n",
    "\n",
    "\n",
    "# Hypothesis:\n",
    "# {{hypothesis}}\n",
    "\n",
    "\n",
    "# Here are one or more Human Explanation/s:\n",
    "# {{human_explanation}}\n",
    "\n",
    "\n",
    "# Model Explanation:\n",
    "# {{model_explanation}}\n",
    "\n",
    "# ---\n",
    "\n",
    "# Output Format:\n",
    "# Wrap your answers using these tags:\n",
    "# - <score> [1-5] </score>\n",
    "# - <justification> [your reasoning in 2–4 sentences] </justification>\n",
    "# \"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b1d56588-19b8-44c3-a85b-4f3a56d01c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE_PROMPT = f\"\"\"\n",
    "You are given a *premise* and a *hypothesis*, followed by one or more human-written explanations and a model-generated explanation.\n",
    "\n",
    "The model explanation is generated based on the premise and hypothesis — specifically, whether the hypothesis logically follows from or contradicts the premise.\n",
    "\n",
    "Your task is to evaluate how well the model explanation aligns with human reasoning.\n",
    "\n",
    "Assess the model explanation on:\n",
    "- Faithfulness to the premise and hypothesis\n",
    "- Similarity to the human-written explanations\n",
    "\n",
    "Assign a score from 1 to 5:\n",
    "- 5: Excellent — Faithful, clear, complete, and closely matches human explanations.\n",
    "- 4: Good — Mostly correct with minor issues.\n",
    "- 3: Average — Partially correct but missing important reasoning.\n",
    "- 2: Poor — Significant mistakes or confusion.\n",
    "- 1: Very Poor — Wrong, misleading, or irrelevant.\n",
    "\n",
    "Then provide a justification in 2–4 sentences comparing the model's explanation to the human explanations.\n",
    "\n",
    "---\n",
    "\n",
    "Premise:\n",
    "{{premise}}\n",
    "\n",
    "Hypothesis:\n",
    "{{hypothesis}}\n",
    "\n",
    "Human Explanation(s):\n",
    "{{human_explanation}}\n",
    "\n",
    "Model Explanation (Generated based on the premise and hypothesis):\n",
    "{{model_explanation}}\n",
    "\n",
    "---\n",
    "\n",
    "Output Format:\n",
    "Wrap your answers using these tags:\n",
    "- <score> [1-5] </score>\n",
    "- <justification> [your reasoning in 2–4 sentences] </justification>\n",
    "\"\"\"\n",
    "\n",
    "def format_judge_prompt(JUDGE_PROMPT, premise, hypothesis, model_explanation, human_explanation):\n",
    "    if isinstance(human_explanation, list):\n",
    "        human_explanation = \"\\n- \" + \"\\n- \".join(human_explanation)\n",
    "    \n",
    "    filled_prompt = JUDGE_PROMPT.replace(\"{premise}\", premise)\\\n",
    "                                 .replace(\"{hypothesis}\", hypothesis)\\\n",
    "                                 .replace(\"{model_explanation}\", model_explanation)\\\n",
    "                                 .replace(\"{human_explanation}\", human_explanation)\n",
    "    return filled_prompt.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e74da07f-1aff-446b-9ed3-2970c2489ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUDGE_PROMPT = f\"\"\" Get a score (1-5) from LLM comparing model explanation to human explanation. You are an expert evaluator of natural language explanations for machine learning model predictions.\n",
    "# Compare the following two explanations for the same data point and assign a score from 0-5\n",
    "# (5 being perfect alignment) based on how well the Model Explanation matches the Human Explanation.\n",
    "# The relationship between the Premise: {{premise}} ; and\n",
    "#         Hypothesis: {{hypothesis}} ; was classified as {{pred}}.\n",
    "\n",
    "\n",
    "# **Human Explanation**: {{human_explanation}}\n",
    "# **Model Explanation**: {{model_explanation}}\n",
    "# Consider:\n",
    "# 1. Clarity and coherence\n",
    "# 2. Coverage of important points\n",
    "# 3. Logical consistency\n",
    " \n",
    "# Return ONLY the numerical score (1-5) with no additional text.\n",
    "# Wrap your answers using these tags:\n",
    "# - <score> [1-5] </score>\n",
    "# - <justification> [your reasoning in 2–4 sentences] </justification> \"\"\"\n",
    "\n",
    "\n",
    "# def format_judge_prompt(JUDGE_PROMPT, premise, hypothesis, pred,human_explanation,model_explanation):\n",
    "#     if isinstance(human_explanation, list):\n",
    "#         human_explanation = \"\\n- \" + \"\\n- \".join(human_explanation)\n",
    "    \n",
    "#     filled_prompt = JUDGE_PROMPT.replace(\"{premise}\", premise)\\\n",
    "#                                  .replace(\"{hypothesis}\", hypothesis)\\\n",
    "#                                  .replace(\"{pred}\", pred)\\\n",
    "#                                  .replace(\"{model_explanation}\", model_explanation)\\\n",
    "#                                  .replace(\"{human_explanation}\", human_explanation)\n",
    "#     return filled_prompt.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "58acc0e0-1aa1-4b11-8cf9-5bf68cc79d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def claude_scorer(judge_prompt):\n",
    "    system_prompt = \"You are an expert judge for Explainable AI (XAI) systems. Your task is to evaluate and score language model explanations based on their faithfulness, clarity, completeness, and similarity to human-written explanations.\"\n",
    "    response = call_claude_on_bedrock(system_prompt, judge_prompt)\n",
    "    return response\n",
    "\n",
    "def get_score(text):\n",
    "    match = re.search(r\"<score>(.*?)</score>\", text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return None\n",
    "\n",
    "def get_justification(text):\n",
    "    match = re.search(r\"<justification>(.*?)</justification>\", text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0894d6e7-37d7-4e95-987d-e7b5b72e40a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179, 179, 179, 179, 179, 179, 179)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "premises = df['premise'].to_list()\n",
    "hypos =  df['hypothesis'].to_list()\n",
    "labels = df['gt_label'].to_list()\n",
    "preds = df['cot_label'].to_list()\n",
    "model_exps =  df['cotfull_response'].to_list() \n",
    "human_exps =  df['human_explanation'].to_list() \n",
    "shap_exps = df['shap_model_explanation'].to_list()  \n",
    "len(premises), len(hypos) , len(labels), len(preds), len(model_exps), len(human_exps), len(shap_exps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5684c273-e6f8-4ea0-91a2-44dceecc8995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['premise', 'hypothesis', 'gt_label', 'pred_label', 'shap_value',\n",
       "       'human_explanation', 'baseline_model_explanation',\n",
       "       'shap_model_explanation', 'cotfull_response', 'cot_explanations',\n",
       "       'cot_label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ad594767-e6a5-4861-9df1-0608afed6f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('**SAMPLE INPUT**\\n    <premise>\\n    Two women are embracing while holding to go packages.\\n    </premise>\\n    \\n    <Hypothesis>\\n    The men are fighting outside a deli.\\n    </Hypothesis>\\n\\n    **SAMPLE OUTPUT**\\n    <explanation>\\n    The men are fighting outside a deli.\\n    </explanation>\\n    \\n    <label>\\n    entailment\\n    </label>\\n\\n### Input Format\\nThe input consists of a sequence of pairs of sentences. Each sentence is represented as a string of tokens.\\n\\n### Output Format\\nThe output should consist of a single line containing the following three elements:\\n\\n1. A string representing the explanation for why you chose the given label.\\n2. A string representing the label chosen.\\n3. A newline character.',\n",
       " 'In the first sentence there is an action of affection between women while on the second sentence there is a fight between men.',\n",
       " '**INPUT FORMAT**\\n    <premise>\\n    [premise]\\n    </premise>\\n    \\n    <Hypothesis>\\n    [hypothesis]\\n    </Hypothesis>\\n    \\n    <shap_tokens>\\n    [shap_tokens]\\n    </shap_tokens>\\n    \\n    **SAMPLE INPUT**\\n    <premise>\\n    Two women are embracing while holding to go packages.\\n    </premise>\\n    \\n    <Hypothesis>\\n    The men are fighting outside a deli.\\n    </Hypothesis>\\n    \\n    <shap_tokens>\\n    .,  packages,  outside, .\\n,  are,  to,  women, i,  The,  are\\n    </shap_')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_exps[0], human_exps[0], shap_exps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3a1f95ab-00e4-4592-86a7-1d054b55c82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a131c2025714cca8033b5227ce314e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results=[]\n",
    "for i in tqdm(range(len(premises))):\n",
    "    premise = str(premises[i])\n",
    "    hypothesis = str(hypos[i])\n",
    "    model_explanation = str(model_exps[i])\n",
    "    shap_exp = str(shap_exps[i])\n",
    "    human_explanation = str(human_exps[i])\n",
    "    true_label = str(labels[i])\n",
    "    predicted_label = str(preds[i])\n",
    "\n",
    "    judge_prompt_base = format_judge_prompt(JUDGE_PROMPT, premise, hypothesis, model_explanation, human_explanation) # format_judge_prompt(JUDGE_PROMPT, premise, hypothesis, predicted_label,human_explanation,model_explanation) #\n",
    "    response_base = claude_scorer(judge_prompt_base)\n",
    "    score_base = get_score(response_base)\n",
    "    just_base = get_justification(response_base)\n",
    "\n",
    "    judge_prompt_shap = format_judge_prompt(JUDGE_PROMPT, premise, hypothesis, shap_exp, human_explanation) #format_judge_prompt(JUDGE_PROMPT, premise, hypothesis, predicted_label,human_explanation,shap_exp) #\n",
    "    response_shap = claude_scorer(judge_prompt_shap)\n",
    "    score_shap = get_score(response_shap)\n",
    "    just_shap = get_justification(response_shap)\n",
    "\n",
    "\n",
    "    \n",
    "    results.append({\n",
    "        \"premise\": premise,\n",
    "        \"hypothesis\": hypothesis,\n",
    "        \"true_label\": true_label,\n",
    "        \"predicted_label\": predicted_label,\n",
    "        \"model_explanation\": model_explanation,\n",
    "        'shapbased_explanation' : shap_exp, \n",
    "        \"human_explanations\": human_explanation,\n",
    "        \"judge_prompt_shap\": judge_prompt_shap,\n",
    "        'score_reasoning':score_base,\n",
    "        'justification_reasoning':just_base,\n",
    "        'score_shap':score_shap,\n",
    "        'justification_shap':just_shap\n",
    "    })\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f8462a6a-ff2f-4f80-b388-ecf2ebc3e25e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Llama_3_8b_inst_explanations', 'Llama_3_8b_inst_explanations_scored.json')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname,f'{fname}_scored.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "10a1f49f-f2ce-4099-a7a3-0a538583b01a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['1', '2', '5', '2', '1'], ['1', None, '1', '1', None])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[entry['score_reasoning'] for entry in results][:5] ,[entry['score_shap'] for entry in results][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1d95adf5-1f54-44c3-862e-010d8ff43f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.0, 1.7988826815642458)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open(f'{fname}_scored.json','r') as f:\n",
    "#     json.dump(f,indent=)\n",
    "def getscoresvalues(data,key):\n",
    "    ret=[]\n",
    "    for entry in data:\n",
    "        try :\n",
    "            ret.append(int(entry[key]))\n",
    "        except:\n",
    "            ret.append(3)\n",
    "    \n",
    "    return ret\n",
    "\n",
    "sb  = sum(getscoresvalues(results,'score_reasoning' ) ) / len(results)\n",
    "shaps  = sum(getscoresvalues(results,'score_shap' ) ) / len(results)\n",
    "sb, shaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60fbf32-a679-449e-891e-344c607bed42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "be695c27-4bc2-40c6-8643-da952bc13011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# name = f'{name}_base.json'\n",
    "# with open(name,'w') as f:\n",
    "#     json.dump(results,f,indent=4)\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(f'{fname}_scored.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cb5c707e-9d40-44ca-a093-c8e15a2e4402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "premise : Two young children in blue jerseys, one with the number 9 and one with the number 2 are standing on wooden steps in a bathroom and washing their hands in a sink. \n",
      "\n",
      "hypothesis : Two kids in numbered jerseys wash their hands. \n",
      "\n",
      "true_label : entailment \n",
      "\n",
      "predicted_label : entailment \n",
      "\n",
      "model_explanation : **SAMPLE INPUT**\n",
      "    <premise>\n",
      "    Two young children in blue jerseys, one with the number 9 and one with the number 2 are standing on wooden steps in a bathroom and washing their hands in a sink.\n",
      "    </premise>\n",
      "    \n",
      "    <Hypothesis>\n",
      "    Two kids in numbered jerseys wash their hands.\n",
      "    </Hypothesis>\n",
      "\n",
      "    **SAMPLE OUTPUT**\n",
      "    <explanation>\n",
      "    The hypothesis is entailed by the premise because it is true that two kids in numbered jerseys wash their hands.\n",
      "    </explanation>\n",
      "    \n",
      "    <label>\n",
      "    entailment\n",
      "    </label> \n",
      "\n",
      "shapbased_explanation : **INPUT FORMAT**\n",
      "    <premise>\n",
      "    [premise]\n",
      "    </premise>\n",
      "    \n",
      "    <Hypothesis>\n",
      "    [hypothesis]\n",
      "    </Hypothesis>\n",
      "    \n",
      "    <shap_tokens>\n",
      "    [shap_tokens]\n",
      "    </shap_tokens>\n",
      "    \n",
      "    **SAMPLE INPUT**\n",
      "    <premise>\n",
      "    Two young children in blue jerseys, one with the number 9 and one with the number 2 are standing on wooden steps in a bathroom and washing their hands in a sink.\n",
      "    </premise>\n",
      "    \n",
      "    <Hypothesis>\n",
      "    Two kids in numbered jerseys wash their hands.\n",
      "    </Hypothesis>\n",
      "    \n",
      "    <shap_tokens \n",
      "\n",
      "human_explanations : Young children are kids. Jerseys with number 9 and 2 are numbered jerseys. \n",
      "\n",
      "score_shap : None \n",
      "\n",
      "justification_shap : None \n",
      "\n"
     ]
    }
   ],
   "source": [
    "d=results[1]\n",
    "for key in ['premise', 'hypothesis', 'true_label', 'predicted_label', 'model_explanation', 'shapbased_explanation','human_explanations','score_shap', 'justification_shap']:\n",
    "    print(f'{key} : {d[key]} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b34e8545-26b2-47f0-ab25-da1d7de8d892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getscores(data,key):\n",
    "    ret=[]\n",
    "    for entry in data:\n",
    "        if entry[key] not in [None,'N/A']:\n",
    "            ret.append(int(entry[key]))\n",
    "        else:\n",
    "            ret.append(0)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fc9df85e-58ce-4f2c-94bb-984241ef368b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m scores \u001b[38;5;241m=\u001b[39mgetscores(results,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore_shap\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m      2\u001b[0m scores\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(scores)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "scores =getscores(results,'score_shap') \n",
    "scores\n",
    "sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "489062b6-7bb8-47f4-a8f4-4488ce4345de",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[0;32m----> 2\u001b[0m counts \u001b[38;5;241m=\u001b[39m Counter(\u001b[43mscores\u001b[49m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(counts)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scores' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(scores)\n",
    "\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e73225c5-3b92-4c68-a5cf-c05b1256252b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['contradiction', 'entailment', 'entailment', 'entailment', 'contradiction']\n",
      "['contradiction', 'entailment', 'entailment', 'entailment', 'contradiction']\n"
     ]
    }
   ],
   "source": [
    "print([entry['true_label'] for entry in results])\n",
    "print([entry['predicted_label'] for entry in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af82901-e50e-4cd5-894e-0b76c1500ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
